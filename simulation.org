* 平滑滤波处理火焰图像数据
** 实验目的
- 消除图像在数字化过程中产生或者混入的噪声，即消除闪烁对摄像机成像的影响。
** 理论基础
1. 图像平滑
   图像增强是对图像进行处理，使其比原始图像更适合于特定的应用，它需要与实际应用相结合。对于图像的某些特征如边缘、轮廓、对比度等，图像增强是进行强调或锐化，以便于显示、观察或进一步分析与处理。图像增强主要是一个主观过程，而图像复原大部分是一个客观过程。图像增强的方法是因应用不同而不同的，研究内容包括：
   #+CAPTION: 图像增强
   [[./img/blur/thesis/img_enhance.png]]
   
   图像平滑是一种区域增强的算法，平滑算法有邻域平均法、中指滤波、边界保持类滤波等。在图像产生、传输和复制过程中，常常会因为多方面原因而被噪声干扰或出现数据丢失，降低了图像的质量（某一像素，如果它与周围像素点相比有明显的不同，则该点被噪声所感染）。这就需要对图像进行一定的增强处理以减小这些缺陷带来的影响。图像平滑主要有均值滤波、高斯滤波、中值滤波和双边滤波等。
   #+CAPTION: 图像滤波
   [[./img/blur/thesis/img_blur.png]]
2. 均值滤波
   均值滤波是指任意一点的像素值，都是周围 N*M 个像素值的均值。例如下图中，红色点的像素值是其周围蓝色背景区域像素值之和除 25， 25=5*5 是蓝色区域的大小。
   #+CAPTION: 均值滤波算法
   [[./img/blur/thesis/mean.png]]
3. 中值滤波
   在使用邻域平均法去噪的同时也使得边界变得模糊。而中值滤波是非线性的图像处理方法，在去噪的同时可以兼顾到边界信息的保留。选一个含有奇数点的窗口 W ，将这个窗口在图像上扫描，把窗口中所含的像素点按灰度级的升或降序排列，取位于中间的灰度值来代替该点的灰度值。计算过程如下图所示：
   #+CAPTION: 中值滤波算法
   [[./img/blur/thesis/median.png]]
4. 高斯滤波
   为了克服简单局部平均法的弊端(图像模糊)，目前已提出许多保持边缘、细节的局部平滑算法。它们的出发点都集中在如何选择邻域的大小、形状和方向、参数加平均及邻域各店的权重系数等。图像高斯平滑也是邻域平均的思想对图像进行平滑的一种方法，在图像高斯平滑中，对图像进行平均时，不同位置的像素被赋予了不同的权重。高斯平滑与简单平滑不同，它在对邻域内像素进行平均时，给予不同位置的像素不同的权值。高斯滤波让临近的像素具有更高的重要度，对周围像素计算加权平均值，较近的像素具有较大的权重值。如下图所示，中心位置权重最高为0.4。
   #+CAPTION: 高斯滤波算法
   [[./img/blur/thesis/gaussian.png]]
5. 双边滤波
   双边滤波器的优点是能够做边缘保存，一般过去用的维纳滤波或者高斯滤波去降噪。都会较明显地模糊边缘，对于高频细节的保护效果并不明显。双边滤波器顾名思义比高斯滤波多了一个高斯方差 sigma－d ，它是基于空间分布的高斯滤波函数。所以在边缘附近，离的较远的像素不会太多影响到边缘上的像素值，这样就保证了边缘附近像素值的保存。
   #+CAPTION: 双边滤波算法
   [[./img/blur/thesis/bilateral.png]]
   
   p点是掩膜中心（黄色圈圈），q点是邻域内某点（白色圈圈），生成了两个核函数，一个空间域高斯核，一个值域高斯核，然后相乘得到最终核函数，对输入进行卷积或相关，得到输出，从图中可以看出，边缘得到了很好的保持，噪声也被抑制。
** 数据来源
- 火电厂视频数据截取的火焰图像。
** 实验步骤
1. 均值滤波 Python调用OpenCV实现的函数如下：

   result = cv2.blur(原始图像, 核大小)

   其中，核大小是以（宽度，高度）表示的元组形式。常见的形式包括：核大小（3，3）和（5，5）。
   代码如下：
   #+CAPTION: 具体代码
   [[./img/blur/flame/code.png]]
2. 中值滤波 Python调用OpenCV实现的函数如下：

   dst = cv2.medianBlur(src, ksize)

   其中，参数：src 表示源图像；ksize 表示核大小。核必须是大于1的奇数，如3、5、7等。
3. 高斯滤波 Python调用OpenCV实现的函数如下：

   dst = cv2.GaussianBlur(src, ksize, sigmaX)

   其中，参数：src 表示原始图像；ksize 表示核大小；sigmaX 表示X方向方差。
   注：核大小（N, N）必须是奇数，X方向方差主要控制权重。sigmax = 0.3*[(ksize-1)*0.5]+0.8 。
4. 双边滤波 Python调用OpenCV实现的函数如下：
   
   dst = = cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]])

   其中，src：输入图像；d：过滤时周围每个像素领域的直径；sigmaColor：在color space 中过滤 sigma。 参数越大，临近像素将会在越远的地方 mix；sigmaSpace：在coordinate space 中过滤 sigma。 参数越大，那些颜色足够相近的的颜色的影响越大。
** 结果分析与讨论
#+CAPTION: 原始图像
[[./img/blur/flame/original.jpg]]

经滤波后的运行结果如下：
1. 均值滤波
   #+CAPTION: k=3; k=5; k=7 
   [[./img/blur/flame/mean_k3.png]]
   [[./img/blur/flame/mean_k5.png]]
   [[./img/blur/flame/mean_k7.png]]
2. 中值滤波
   #+CAPTION: k=3; k=5
   [[./img/blur/flame/median_k3.png]]
   [[./img/blur/flame/median_k5.png]]
3. 高斯滤波
   #+CAPTION: k=3; k=5; k=7
   [[./img/blur/flame/gaussian_k3.png]]
   [[./img/blur/flame/gaussian_k5.png]]
   [[./img/blur/flame/gaussian_k7.png]]
4. 双边滤波
   #+CAPTION: d=9
   [[./img/blur/flame/bilateral.png]]
5. 滤波后的图片resize为符合模型特性的长宽相等的正方形图片：
   #+CAPTION: resize: 128*128*3
   [[./img/blur/flame/resize.png]]
** 结论
1. 线性滤波随着核大小逐渐变大，图像变得更加模糊，但均不丢失图像的关键信息。
2. 用cv2.resize()函数将滤波后的图片转化为正方形图片时，图片噪声较大。采用k=7的高斯滤波时，图片噪声明显减小。
* Convolutional Sparse Autoencoder
** 实验目的
利用具有深层结构的CSAE网络提取火焰图像的关键特征
** 理论基础
1. 稀疏自编码器（Sparse Autoencoder）
   1. 稀疏自动编码器(SAE)其实就是在普通autoencoder的基础上增加了稀疏的约束，其中稀疏惩罚项常用为L1/L2正则化，其未限制网络接收数据的能力，即不限制隐藏层的单元数，且使得神经网络在隐藏层神经元较多的情况下依然能够提取样本的特征和结构。
      稀疏自编码器的基本模型是一个三层的神经网络，在学习时让网络输出的目标值接近于输入的图像本身，从而学习图像中的特征。
      所谓稀疏性限制是指：。若激活函数是tanh，则当神经元的输出接近于-1的时候认为神经元是被抑制的。
      #+CAPTION: 稀疏自编码实例图
      [[./img/autoencoder/thesis/Sparse_AE_1.png]] \\
      如上图所示，浅色的神经元表示被抑制的神经元，深色的神经元表示被激活的神经元。通过稀疏自编码器，没有限制隐藏层的单元数，但防止了网络过度记忆的情况。

      稀疏自编码器损失函数的基本表示形式如下：
      #+CAPTION: 稀疏自编码器损失函数的基本表示形式
      [[./img/autoencoder/thesis/Sparse_AE_2.png]] \\
      其中g(h)g(h)是解码器的输出，通常h是编码器的输出，即h=f(x)。
   2.
      稀疏性
      一般使得隐含层小于输入结点的个数，但是我们也可以让隐藏层的节点数大于输入结点的个数，只需要对其加入一定的稀疏限制就可以达到同样的效果。如何隐藏层的节点中大部分被抑制，小部分被激活，这就是稀疏。当神经元的的输出接近激活函数上限时（例如对于Sigmoid为1）称该神经元状态为激活，反之当神经元的输出接近激活函数的下限时称该神经元的状态为抑制，那么当某个约束或规则使得神经网络中大部分的神经元的状态为抑制时，称该约束为“稀疏性限制”；如果采用tanh函数，当神经元的输出接近1时为激活，接近-1时为稀疏。
      稀疏自动编码希望让隐含层的平均激活度为一个比较小的值，隐含层的平均激活的数据表示为：
      #+CAPTION: 隐含层平均激活度
      [[./img/autoencoder/csae/thesis/csae_1.png]]
      其中，表示在输入数据为x的情况下，隐藏神经元j的激活度。
      为了使得均激活度为一个比较小的值，引入\[rho] ，称为稀疏性参数，一般是一个比较小的值，使得\[rho_{j}=rho]这样就可以是隐含层结点的活跃度很小。
   3. 损失函数
      上面只是理论上的解释，为了转化成数据表示引入KL散度（KL divergence），即相对熵，使得隐含层结点的活跃度很小。基本的想法是让约束值 ρ_hat 等于稀疏参数 ρ。具体实现时在原始损失函数中增加表示稀疏性的正则项，损失函数如下：
      loss = Mean_squared_error + Regularization_for_sparsity_parameter  ( 损失 = 均方误差 + 稀疏参数正则项 )
      如果 ρ_hat 偏离 ρ，那么正则项将惩罚网络，一个常规的实现方法是衡量 ρ 和 ρ_hat 之间的 Kullback-Leiber (KL) 散度。KL散度是衡量两个分布之间差异的非对称度量，当 ρ 和 ρ_hat 相等时，KL 散度是零，否则会随着两者差异的增大而单调增加，KL 散度的数学表达式如下：
      #+CAPTION: KL散度公式
      [[./img/autoencoder/csae/thesis/KL_function.png]]
      假设  ρ=0.3 时的 KL 的散度 DKL的变化图，从图中可以看到，当 ρ_hat=0.3时，DKL=0；而在 0.3 两侧都会单调递增：
      #+CAPTION: DKL变化图
      [[./img/autoencoder/csae/thesis/KL_DKL.png]]
      根据这个性质，我们就可把相对熵加入到损失中，惩罚平均激活度离 比较远的值，使得最后学习得到参数能够让平均激活度保持在这个水平。在隐含层加上稀疏约束后，损失函数为：
      #+CAPTION: 稀疏损失函数
      [[./img/autoencoder/csae/thesis/J_sparse.png]]
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 导入必要的模块；
2. 自定义load_data()函数导入火焰图像数据；
3. 搭建计算均值和方差的编码和生成图像的解码CNN神经网络；
4. 定义CSAE损失函数，引入KL约束条件；
5. 调用fit()进行训练；
6. 查看编码前后对比图像；
7. 保存实验结果: 1024维中间层变量。
** 结果分析与讨论
1. 编码前后对比图：
   #+CAPTION: 重构图像
   [[./img/autoencoder/csae/result/csae_contrastion.png]]
2. 训练集和测试集的损失值：
   #+CAPTION: 损失值
   [[./img/autoencoder/csae/result/sparse_val_loss_1.png]]
   [[./img/autoencoder/csae/result/sparse_val_loss_2.png]]
3. 分析：
   经模型编码后生成的图像基本上保留了火焰图像的关键信息；损失值随着训练加深逐步下降。
** 结论
相较于vae模型，loss下降更加稳定，波动较小；生成的重构图像较为模糊但保留图像数据的基本关键信息。

下一步：
1. 数据处理（将火焰视频转化为每秒25帧的火焰图像，同时为消除闪烁对摄像机成像的影响，对图像进行平滑处理，即利用图像处理技术对每秒得到的25张图片进行取平均、平均值滤波，以代表该时间段内的火焰燃烧状态。根据卷积自编码器特性，将输入图片转化为长宽相等的正方形图片作为训练数据。）
2. 训练csae模型
3. 提取特征
* AutoEncoder(MNIST手写数字集)
** 实验目的
运用自编码器重构输出（keras框架下实现）
** 理论基础
1. 自编码器（Autoencoder, AE）
   1. 自编码器是一种无监督的数据维度压缩和数据特征表达方法，利用反向传播算法使得输出值等于输入值的神经网络，先将输入压缩成潜在空间表征，然后通过这种表征来重构输出。
      #+CAPTION: 自编码器结构
      [[./img/autoencoder/thesis/AE_1.png]] \\
      自编码器由两部分组成：
      - 编码器（encoder）：这部分能将输入压缩成潜在空间表征，可以用编码函数h=f(x)表示。
      - 解码器（decoder）：这部分重构来自潜在空间表征的输入，可以用解码函数r=g(h)表示。

      因此，整个自编码器可以用函数g(f(x))=r来描述，其中输出r与原始输入x相近。h=f(x)表示编码器，r=g(h)=g(f(x))表示解码器，自编码的目标便是优化损失函数L(x,g(f(x))，也就是减小图中的Error。
   2. 自编码器是前馈神经网络的一种，最开始主要用于数据的降维以及特征的抽取，随着技术的不断发展，现在也被用于生成模型中，可用来生成图片等。前馈神经网络是有监督学习，其需要大量的标注数据。自编码器是无监督学习，数据不需要标注因此较容易收集。前馈神经网络在训练时主要关注的是输出层的数据以及错误率，而自编码的应用可能更多的关注中间隐层的结果。
   3. 在普通的自编码器中，输入和输出是完全相同的，因此输出没有什么应用价值，所以我们希望利用中间隐层的结果，比如，可以将其作为特征提取的结果、利用中间隐层获取最有用的特性等。但是如果只使用普通的自编码器会面临什么问题呢？比如，输入层和输出层的维度都是5，中间隐层的维度也是5，那么使用相同的输入和输出来不断优化隐层参数，最终得到的参数可能是：x1−>a1，x2−>a2，… 的参数为1，其余参数为0。也就是说，中间隐层的参数只是完全将输入记忆下来，并在输出时将其记忆的内容完全输出即可，神经网络在做恒等映射，产生数据过X拟合。
      #+CAPTION: 自编码器的完全记忆情况
      [[./img/autoencoder/thesis/AE_2.png]] \\ 如图是隐层单元数等于输入维度的情况，如果是隐层单元数大于输入维度也会发生类似的情况，即当隐层单元数大于等于输入维度时，网络可以采用完全记忆的方式。虽然这种方式在训练时精度很高，但是复制的输出无实际意义。因此，往往给隐层加一些约束，如限制隐藏单元数、添加正则化等。
2. 栈式自编码器（Stack Autoencoder）
   1. 栈式自编码器又称为深度自编码器，其训练过程和深度神经网络有所区别，下面是基于栈式自编码器的分类问题的训练过程（图片来自台大李宏毅老师的PPT）：
      #+CAPTION: 栈式自编码器的训练过程
      [[./img/autoencoder/thesis/Stack_AE_1.png]]
   2. 训练过程：首先，训练784->1000->784的自编码器，而后已经固定已经训练好的参数和1000维的结果，训练第二个自编码器：1000->1000->1000，而后固定已经训练好的参数和训练的中间层结果，训练第三个自编码器：1000->500->1000，固定参数和中间隐层的结果。此时，前3层的参数已经训练完毕，此时，最后一层接一个分类器，将整体网络使用反向传播进行训练，对参数进行微调。这便是使用栈式自编码器进行分类的整体过程。（encoder和decoder的参数可以是对称的，也可以是非对称的）
   3. 栈式自编码器增加隐层可以学到更复杂的编码，每一层可以学习到不同的信息维度。若层数太深，encoder过于强大，可以将学习将输入映射为任意数（然后decoder学习其逆映射）。这一编码器可以很好的重建数据，但并没有在这一过程中学到有用的数据表示。
3. 稀疏自编码器（Sparse Autoencoder）
   1. 稀疏自编码器是加入正则化的自编码器，其未限制网络接收数据的能力，即不限制隐藏层的单元数。
      所谓稀疏性限制是指：若激活函数是sigmoid，则当神经元的输出接近于1的时候认为神经元被激活，输出接近于0的时候认为神经元被抑制。使得大部分神经元别抑制的限制叫做稀疏性限制。若激活函数是tanh，则当神经元的输出接近于-1的时候认为神经元是被抑制的。
      #+CAPTION: 稀疏自编码实例图
      [[./img/autoencoder/thesis/Sparse_AE_1.png]] \\
      如上图所示，浅色的神经元表示被抑制的神经元，深色的神经元表示被激活的神经元。通过稀疏自编码器，没有限制隐藏层的单元数，但防止了网络过度记忆的情况。

      稀疏自编码器损失函数的基本表示形式如下：
      #+CAPTION: 稀疏自编码器损失函数的基本表示形式
      [[./img/autoencoder/thesis/Sparse_AE_2.png]] \\
      其中g(h)g(h)是解码器的输出，通常h是编码器的输出，即h=f(x)。
   2. 损失函数和BP函数推导
      （暂略）
   3. 稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。
      #+CAPTION: 自编码器在分类上的应用
      [[./img/autoencoder/thesis/Sparse_AE_3.png]] \\
      上图所述过程不是一次训练的，可以看到上面只有编码器没有解码器，因此其训练过程是自编码器先使用数据训练参数，然后保留编码器，将解码器删除并在后面接一个分类器，并使用损失函数来训练参数已达到最后效果。
4. 去噪自编码器（Denoising Autoencoder）
   1. 去噪自编码器是一类接受损失数据作为输入，并训练来预测原始未被损坏的数据作为输出的自编码器。
      #+CAPTION: 去噪自编码器代价函数计算图
      [[./img/autoencoder/thesis/Denoising_AE_1.png]]
   2. 训练过程：引入一个损坏过程 C(\tilde{x}|x)，这个条件分布代表给定数据样本x产生损坏样本\tilde{x}的概率。自编码器学习重构分布p_{reconstruct}(x|\tilde{x})：
      - 从训练数据中采一个训练样本x
      - 从C(\tilde{x}|X=x)采一个损坏样本\tilde{x} 
      - 将(\tilde{x}, x)作为训练样本来估计自编码器的重构分布p_{reconstruct}(x|\tilde{x})=p_{decoder}(x|h)，其中h是编码器f(\tilde{x})的输出，p_{decoder}p根据解码函数g(h)定义。

      去噪自编码器中作者给出的直观解释是：和人体感官系统类似，比如人的眼睛看物体时，如果物体的某一小部分被遮住了，人依然能够将其识别出来，所以去噪自编码器就是破坏输入后，使得算法学习到的参数仍然可以还原图片。
   3. 普通的自编码器的本质是学一个相等函数，即输入和输出是同一个内容，这种相等函数的缺点便是当测试样本和训练样本不符合同一个分布时，在测试集上效果不好，而去噪自编码器可以很好地解决这个问题。欠完备自编码器限制学习容量，而去噪自编码器允许学习容量很高，同时防止在编码器和解码器学习一个无用的恒等函数。经过了加入噪声并进行降噪的训练过程，能够强迫网络学习到更加鲁棒的不变性特征，获得输入的更有效的表达。
5. 卷积自编码器（Convolutional Autoencoder）
   1. 卷积自编码器和普通自编码器的区别在于其encoder和decoder都是卷积神经网络，相应的，encoder使用的是卷积操作和池化操作，而decoder中使用的反卷积操作和反卷积操作。
      （关于卷积、反卷积、池化和反池化的内容暂略）
** 数据来源
MNIST手写数字集
** 实验步骤
1. 自编码器（Autoencoder, AE） \\
   Keras封装的比较厉害，这里是最简单的自编码器，其输入维度是28*28=784，中间单隐层的维度是2，使用的激活函数是Relu，返回encoder和autoencoder。encoder部分可以用于降维后的可视化，或者降维之后接分类等，autoencoder可以用来生成图片等。
   结构见图如下：
   #+CAPTION: 自编码器代码结构图
   [[./img/autoencoder/thesis/AE_3.png]]
2. 栈式自编码器（Stack Autoencoder） \\
   栈式自编码器相当于深度网络的过程，主要注意维度对应即可，另外，这里设置的encoder和decoder的维度是对称的。
   其架构图如下：
   #+CAPTION: 栈式自编码器代码架构
   [[./img/autoencoder/thesis/Stack_AE_2.png]]
3. 稀疏自编码器（Sparse Autoencoder） \\
   以多层的自编码器举例，单隐层的同样适用，主要是在第一层加一个正则化项，activity_regularizer=regularizers.l1(10e-6)说明加入的是L1正则化项，10e-6是正则化项系数。
   其架构如下：
   #+CAPTION: 稀疏自编码器代码架构
   [[./img/autoencoder/thesis/Sparse_AE_4.png]]
4. 去噪自编码器（Denoising Autoencoder） \\
   去噪自编码器主要是对输入添加噪声，所以训练过程是不需要改变的，只需要改变输入和输出。
   上述便是对输入添加噪声的过程，NOISE_FACTOR * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)便是添加的噪声。 np.clip()是截取函数，将数值限制在0~1之间。
   其架构如下：
   #+CAPTION: 去噪自编码器代码架构
   [[./img/autoencoder/thesis/Denoising_AE_2.png]]
5. 卷积自编码器（convolutional Autoencoder） \\
   在Keras编码中，反卷积的实现代码便是卷积操作。UpSampling2D()实现的是反平均卷积的操作。 
   代码架构图如下：
   #+CAPTION: 卷积自编码器代码架构 
   [[./img/autoencoder/thesis/Conv_AE.png]]
** 结果分析与讨论
1. 自编码器（Autoencoder, AE） \\
   Encoder结果的可视化如图：
   #+CAPTION: 自编码器 Encoder 输出可视化
   [[./img/autoencoder/MNIST/AE_Output_visualization.png]] \\
   上图中不同表示表示不同的数字，由图可知，自编码器降维之后的结果并不能很好地表示10个数字。
   
   AutoEncoder还原之后的图片和原图片对比如下：
   #+CAPTION: 自编码器原图片和生成的图片对比
   [[./img/autoencoder/MNIST/AE_restruction.png]] \\
   上图说明，autoencoder的生成结果不是很清晰。
2. 栈式自编码器（Stack Autoencoder） \\
   Encoder结果的可视化如图：
   #+CAPTION: 栈式自编码器 Encoder 输出可视化
   [[./img/autoencoder/MNIST/Stack_AE_Output_visualization.png]] \\
   上图中不同表示表示不同的数字，由图可知，栈式自编码器的效果相比较普通自编码器好很多，这里基本能将10个分类全部分开。
   
   AutoEncoder还原之后的图片和原图片对比如下：
   #+CAPTION: 栈式自编码器原图片和生成的图片对比
   [[./img/autoencoder/MNIST/Stack_AE_restruction.png]]
3. 稀疏自编码器（Sparse Autoencoder） \\
   Encoder结果的可视化如图：
   #+CAPTION: 稀疏自编码器 Encoder 输出可视化
   [[./img/autoencoder/MNIST/Sparse_AE_Output_visualization.png]] \\
   上图中不同颜色表示不同的数字，由图可知，这个编码器的分类效果还可以，比自编码器好很多，但作用不大，大部分作用需要归功于栈式自编码器。
   
   AutoEncoder还原之后的图片和原图片对比如下：
   #+CAPTION: 稀疏自编码器原图片和生成的图片对比
   [[./img/autoencoder/MNIST/Stack_AE_restruction.png]]
4. 去噪自编码器（Denoising Autoencoder） \\
   Encoder结果的可视化如图：
   #+CAPTION: 去噪自编码器 Encoder 输出可视化
   [[./img/autoencoder/MNIST/Denoising_AE_Output_visualization.png]]
   
   上图中不同表示表示不同的数字，这里不是很直观，看下面的图片对比：
   #+CAPTION: 去噪自编码器原图片和生成的图片对比
   [[./img/autoencoder/MNIST/Denoising_AE_add_noise.png]]
   上图是添加噪声的效果对比，第一行表示原数据，第二行表示噪声处理过后的数据。
   
   AutoEncoder还原之后的图片和原图片对比如下：
   #+CAPTION: 添加噪声前后对比图
   [[./img/autoencoder/MNIST/Denoising_AE_restruction.png]] \\
   上图根据噪声数据还原图片的对比，第一行表示噪声处理过后的数据，第二行表示去噪自编码器decoder还原之后的结果，上图可看出去噪自编码器的效果不错。
5. 卷积自编码器（convolutional Autoencoder） \\
   AutoEncoder还原之后的图片和原图片对比如下：
   #+CAPTION: 卷积自编码器原图片和生成图片对比
   [[./img/autoencoder/MNIST/Conv_AE_restruction.png]] \\
   上图根据原图片和生成图片的对比，第一行表示原图片，第二行表示卷积自编码器decoder还原之后的结果，上图可看出效果不错。
   
   LOSS变化图：
   #+CAPTION: loss 变化图
   [[./img/autoencoder/MNIST/Conv_AE_training_loss.png]]
   #+CAPTION: accuracy 变化图
   [[./img/autoencoder/MNIST/Conv_AE_training_accuracy.png]] \\
   由上图可以看出，虽然 loss 在不断降低，但 accuracy 还不是非常高，一方面是和参数相关， epochs 设置为 20 ，另外，网络的深度也不够，也没有加入一些其他的提高性能的小技巧。
** 结论
实验中用到的几种自编码器的变形对于不同数字的表示效果明显好于普通自编码器，基本能将10个分类分开；而且重构生成结果也较好。
以下效果需要着重考虑：
1. 可比性（不同自编码器之间以及其对火焰数据的适用程度）
2. 适用性（实验逻辑性出发点即对应要解决的具体问题和希望提升的效果）
* CNNmatching模型提取火焰信息
** 实验目的
利用深度学习中CNN神经网络对图片进行匹配的模型，对火焰图像进行处理匹配特征点。
** 理论基础
卷积神经网络(Convolutional Neural Networks, CNN), 由纽约大学的Yann　LeCun于1998年提出，CNN中层次之间的紧密联系和空间信息使得其特别适用于图像的处理和理解，并且能够自动的从图像抽取出丰富的相关特性。CNN是一种深度的监督学习下的机器学习模型，具有极强的适应性，善于挖掘数据局部特征，提取全局训练特征和分类，它的权值共享结构网络使之更类似于生物神经网络，在模式识别各个领域都取得了很好的成果。
1. 稀疏连接：在BP神经网络中，每一层的神经元节点是一个线性一维排列结构，层与层各神经元节点之间是全连接的。卷积神经网络中，层与层之间的神经元节点不再是全连接形式，利用层间局部空间相关性将相邻每一层的神经元节点只与和它相近的上层神经元节点连接，即局部连接。这样大大降低了神经网络架构的参数规模。
2. 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。
3. 最大池采样：它是一种非线性降采样方法。在通过卷积获取图像特征之后是利用这些特征进行分类。可以用所有提取到的特征数据进行分类器的训练，但这通常会产生极大的计算量。所以在获取图像的卷积特征后，要通过最大池采样方法对卷积特征进行降维。将卷积特征划分为数个n*n的不相交区域，用这些区域的最大(或平均)特征来表示降维后的卷积特征。这些降维后的特征更容易进行分类。
4. Softmax回归：它是在逻辑回归的基础上扩张而来，它的目的是为了解决多分类问题。在这类问题中，训练样本的种类一般在两个以上。Softmax回归是有监督学习算法，它也可以与深度学习或无监督学习方法结合使用。

针对深度遥感影像在成像方式，时间相位和分辨率上的差异使得匹配困难的问题，提出了一种新的深度学习特征匹配方法，其特征提取的主要思想和代码均基于D2-Net。
** 数据来源
示例程序源数据（一组名为“df-sm-data”的测试数据，包括来自星载SAR和可见光传感器的图像，无人机热红外传感器以及Google Earth图像）；火电厂视频数据截取的火焰图像。
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像。
2. 将处理后的火焰图像输入到cnn-matching模型中。
3. 通过运行以wget https://dsmn.ml/files/d2-net/d2_tf.pth -O models/d2_tf.pth命令下载现成的VGG16权重及其已调整的对应权重。
4. 利用CNN模型提取图像特征，torch下的DenseFeatureExtractionModule模型结构如下：
   [[./img/cnn-matching/DenseFeatureExtractionModule.png]]
5. 利用Flann特征匹配处理所提取的图像特征，包括匹配对筛选、统计平均距离差、自适应阈值。
6. 输出最终匹配结果，并绘制匹配连线。
** 结果分析与讨论
1. 谷歌地球图像之间的匹配结果（2009年和2018年）:
   [[./img/cnn-matching/reslut_1.jpeg]]
2. 无人机光学图像与红外热像的匹配结果:
   [[./img/cnn-matching/reslut_2.jpeg]]
3. SAR图像（GF-3）与光学卫星（ZY-3）图像的匹配结果:
   [[./img/cnn-matching/reslut_3.jpeg]]
4. 卫星图与地图的匹配结果:
   [[./img/cnn-matching/reslut_4.jpeg]]
5. 火焰图像相邻前后帧的匹配结果：
   [[./img/cnn-matching/result_512.png]]
   [[./img/cnn-matching/result_523.png]]
   [[./img/cnn-matching/result_612.png]]
   [[./img/cnn-matching/result_623.png]]
6. 输入同一帧火焰图像的匹配结果：
   [[./img/cnn-matching/result_5.png]]
   [[./img/cnn-matching/result_6.png]]
** 结论
该算法具有较强的适应性和鲁棒性，在匹配点的数量和分布，效率和适应性方面均优于其他算法。但对于前后帧火焰图像火焰纹理的特征点抓取不够理想，输入为同一帧的火焰图像时效果明显提升。
* SIFT算法提取火焰信息(将灰度矩阵用线性插值处理)
** 实验目的
在python+openCV环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将火焰图像进行灰度化处理
4. 将火焰图像进行增强处理
5. 将处理后的火焰图像输入到SIFT模型中
6. 计算出SIFT的关键点和描述符。
7. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
8. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
9. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
10. 在场景图像中绘制检测到的模板。
11. 绘制SIFT关键点匹配。
** 结果分析与讨论
*** 灰度化
确定灰度值的max和min并设置为上下限，然后对其他像素点的灰度值进行线性插值

处理前[[./img/SIFT/gray1.png]]

处理后[[./img/SIFT/test1.png]]

输入到模型后无法提取到有用信息，提示“Not enough matches are found”
*** 增强处理
1. 先用高斯滤波处理图像，再增强图像对比度，再进行灰度值变换，然后进行空间域kirsch锐化
   1) 具体流程：[[./img/SIFT/chuliguocheng1.png]]
   2) 处理前：[[./img/SIFT/orgin1.png]]
   3) 处理后：[[./img/SIFT/enhance11.png]] 
   4) 输入到模型训练结果[[./img/SIFT/enhance_SIFT1.png]]
2. 先用掩码对图片进行裁剪后转为灰度图，再用高斯滤波处理图像，接着对其增强对比度，再进行灰度值线性变换，然后进行空间域Kirsch锐化
   1) 具体处理流程：[[./img/SIFT/chuliguocheng2.png]]
   2) 处理前[[./img/SIFT/origin1.jpg]]
   3) 处理后[[./img/SIFT/enhance1.png]]
   4) 输入到模型训练结果为[[./img/SIFT/enhance_SIFT2.png]]

由实验结果可看出，模型提取到的主要为边缘轮廓的特征点，对火焰的边缘仅有非常有限的捕捉
*** 输入相同图片
为了验证模型的提取能力，输入同一张的图进行训练，观察其提取特征点的能力
1. 灰度处理的图片输入后仍然无法提取到有用信息，提示“Not enough matches are found”
2. 第一种增强处理后的相同图片输入后，训练结果为[[./img/enhance_SIFT_same1.png]]
3. 第二种增强处理后的相同图片输入后，训练结果为[[./img/enhance_SIFT_same2.png]]
** 结论
经过处理的火焰图像输入到该模型中提取到的信息无法满足课题要求，可考虑更换模型，或调整处理图像的方法。
* SIFT算法提取火焰信息(将图像进行灰度化和二值化处理)
** 实验目的
在python+openCV环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将火焰图像进行灰度化处理
4. 将火焰图像进行二值化处理
5. 将处理后的火焰图像输入到SIFT模型中
6. 计算出SIFT的关键点和描述符。
7. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
8. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
9. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
10. 在场景图像中绘制检测到的模板。
11. 绘制SIFT关键点匹配。
** 结果分析与讨论
1. 火焰图像灰度化结果：[[./img/SIFT/0339_gray.PNG]]
2. 火焰图像二值化结果：[[./img/SIFT/0339_binary.PNG]]
3. 截取后的火焰图像灰度化结果：[[./img/SIFT/0339_crop_gray.PNG]]
4. 截取后的火焰图像二值化结果：[[./img/SIFT/0339_crop_binary.PNG]]
5. 将火焰图像进行灰度化后输入到模型中无法提取到前后帧图像数据的相似点；
6. 将火焰图像二值化后火焰信息丢失严重，无法作为有用数据输入到模型中。
** 结论
经过处理的火焰图像输入到该模型中无法提取火焰信息，可考虑更换模型，或调整二值化的方法。
* SIFT算法提取火焰信息
** 实验目的
在python环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将处理后的火焰图像输入到SIFT模型中
4. 计算出SIFT的关键点和描述符。
5. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
6. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
7. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
8. 在场景图像中绘制检测到的模板。
9. 绘制SIFT关键点匹配。
** 结果分析与讨论
[[./img/SIFT/sift_test_result_1.png]]

该模型不能有效地提取到火焰信息
** 结论
该SIFT模型不能运用到提取火焰信息中，可考虑其他SIFT模型，或openCV的其他特征提取的方法
* SIFT特征匹配的实现
** 实验目的
在python环境下，使用SIFT算法提取图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
1. 示例代码所用的原数据
2. 手机拍摄的图片数据
** 实验步骤
1. 计算出SIFT的关键点和描述符。
2. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
3. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
4. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
5. 在场景图像中绘制检测到的模板。
6. 绘制SIFT关键点匹配。
** 结果分析与讨论
1. 示例代码数据
   [[./img/SIFT/sift_test_result_1.png]]

2. 手机拍摄图片数据
   [[./img/SIFT/sift_test_result_2.png]]

从两个数据的实验结果可看出，该实现基本上可对两张图片的相似点进行较好的提取，但对干扰点的排除有待加强
** 结论
该实现计算出SIFT的关键点和描述符后，对FLANN进行初始化，并用FLANN进行快速高效匹配，通过描述符的距离进行选择需要的点，然后对两张图片的相似点进行匹配连线。
可以考虑是否可运用到火焰图像的相似点检测上。
* 实验名称
** 实验目的（本试验的目的，一定要简单明了）
** 理论基础（说明理论的前提假设有哪些，列出具体步骤）
** 数据来源（说明数据来源，如果是火电厂历史数据，一定要写明电厂名称、时间范围、采样间隔）
** 实验步骤（列出做了哪些事，每件事情与研究内容的联系，以及之间是否存在联系）
** 结果分析与讨论（对每个试验结果进行分析，说明从试验结果得到的信息）
** 结论（列出试验取得的结论）
   
